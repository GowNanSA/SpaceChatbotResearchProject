# -*- coding: utf-8 -*-
"""LlamaChatbotModelStars.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16gb2an0fSNvJVjpj4HA6y6ixBAQ20-ew
"""

# load 1 daatset first
from google.colab import drive

drive.mount('/content/drive')

# https://www.kaggle.com/code/ysthehurricane/step-by-step-huggingface-model-fine-tune-guide

#Llama2 is too big :( and trying to use it made me run out of memory very fast so we are using a kaggle guide to finetune a model to the data

import keras
import sklearn
import pandas as pd
import numpy as np
from keras import preprocessing
from keras import models
from keras import layers
!pip install pytorch_lightning
!ls drive/'My Drive'/'Colab Notebooks'/'DATA'/'exoplanets.csv'

df = pd.read_csv("drive/My Drive/Colab Notebooks/DATA/exoplanets.csv", header=[0])

dfvals = df.values

X = dfvals[:, :-1]
y = dfvals[:, -1]
df.head()

# prompt: git clone https://huggingface.co/meta-llama/Llama-2-7b

from google.colab import drive
import keras
import sklearn
import pandas as pd
import numpy as np
from keras import preprocessing
from keras import models
from keras import layers

# load 1 dataset first

drive.mount('/content/drive')

!ls drive/'My Drive'/'Colab Notebooks'/'DATA'/'exoplanets.csv'

# read first dataset
df = pd.read_csv("drive/My Drive/Colab Notebooks/DATA/exoplanets.csv", header=[0])

dfvals = df.values

X = dfvals[:, :-1]
y = dfvals[:, -1]

df.head()

from huggingface_hub import notebook_login

#notebook_login()

from sklearn.model_selection import train_test_split


import torch

from transformers import T5Tokenizer
from transformers import T5ForConditionalGeneration, AdamW

import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint

pl.seed_everything(100)

import warnings
warnings.filterwarnings("ignore")
print("Number of records: ", df.shape[0])
# Apply .str.lower() to all string columns in the DataFrame
for column in df.select_dtypes(include="object").columns:
    df[column] = df[column].str.lower()

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
INPUT_MAX_LEN = 512 # Input length
OUT_MAX_LEN = 128 # Output Length
TRAIN_BATCH_SIZE = 8 # Training Batch Size
VALID_BATCH_SIZE = 2 # Validation Batch Size
EPOCHS = 5 # Number of Iteration

MODEL_NAME = "t5-base"

tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, model_max_length= INPUT_MAX_LEN)
print("eos_token: {} and id: {}".format(tokenizer.eos_token, tokenizer.eos_token_id)) # End of token (eos_token)
print("unk_token: {} and id: {}".format(tokenizer.unk_token, tokenizer.eos_token_id)) # Unknown token (unk_token)
print("pad_token: {} and id: {}".format(tokenizer.pad_token, tokenizer.eos_token_id)) # Pad token (pad_token)

class T5Dataset:
    def __init__(self, input_data, target_data):
        self.input_data = input_data
        self.target_data = target_data
        self.tokenizer = tokenizer
        self.input_max_len = INPUT_MAX_LEN
        self.out_max_len = OUT_MAX_LEN

    def __len__(self):
        return len(self.input_data)

    def __getitem__(self, item):
        input_text = str(self.input_data[item])
        target_text = str(self.target_data[item])

        # Tokenizing inputs
        inputs_encoding = self.tokenizer(
            input_text,
            add_special_tokens=True,
            max_length=self.input_max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt"
        )

        # Tokenizing targets
        output_encoding = self.tokenizer(
            target_text,
            add_special_tokens=True,
            max_length=self.out_max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt"
        )

        inputs_ids = inputs_encoding["input_ids"].flatten()
        attention_mask = inputs_encoding["attention_mask"].flatten()
        labels = output_encoding["input_ids"]

        labels[labels == 0] = -100  # Mask padding tokens as per T5 documentation

        labels = labels.flatten()

        return {
            "input_ids": inputs_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

class T5DatasetModule(pl.LightningDataModule):
    def __init__(self, train_input, train_target, val_input, val_target):
        super().__init__()
        self.train_input = train_input
        self.train_target = train_target
        self.val_input = val_input
        self.val_target = val_target
        TRAIN_BATCH_SIZE = 4  # Reduce the batch size (adjust based on memory)
        VALID_BATCH_SIZE = 4  # Same for validation batch size


    def setup(self, stage=None):
        # Set up the train and validation datasets
        self.train_dataset = T5Dataset(self.train_input, self.train_target)
        self.val_dataset = T5Dataset(self.val_input, self.val_target)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=16, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=16)

class T5Model(pl.LightningModule):

    def __init__(self):
        super().__init__()
        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

    def forward(self, input_ids, attention_mask, labels=None):
        output = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        return output.loss, output.logits

    def training_step(self, batch, batch_idx):
        # Corrected from 'inputs_ids' to 'input_ids'
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]  # Corrected from 'targets' to 'labels'

        loss, outputs = self(input_ids, attention_mask, labels)

        self.log("train_loss", loss, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        # Corrected from 'inputs_ids' to 'input_ids'
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]  # Corrected from 'targets' to 'labels'

        loss, outputs = self(input_ids, attention_mask, labels)

        self.log("val_loss", loss, prog_bar=True, logger=True)
        return loss

    def configure_optimizers(self):
        return AdamW(self.parameters(), lr=0.0001)

from torch.utils.data import DataLoader

def preprocess_dataframe(df):
    """
    Preprocess the DataFrame by combining all columns into a single 'input' column,
    and optionally generating a 'target' column.
    """
    df = df.fillna("none")

    for column in df.select_dtypes(include="object").columns:
        df[column] = df[column].apply(lambda x: " ".join(str(x).split()) if isinstance(x, str) else str(x))

    # Combine all columns into a single text field
    df["input"] = df.apply(lambda row: " ".join(row.astype(str)), axis=1)

    # Optionally, define 'target' column (e.g., same as 'input' or some other column)
    df["target"] = df["input"]  # Or modify as needed for your task

    return df

# Define the run function
def run():
    preprocessed_df = preprocess_dataframe(df)


    df_train, df_valid = train_test_split(preprocessed_df[0:10000], test_size=0.2, random_state=101)

    df_train = df_train.reset_index(drop=True)
    df_valid = df_valid.reset_index(drop=True)


    dataModule = T5DatasetModule(df_train["input"], df_train["target"], df_valid["input"], df_valid["target"])
    dataModule.setup()

    # Set up device and model
    device = DEVICE
    models = T5Model()
    models.to(device)

    # Configure checkpointing
    checkpoint_callback = ModelCheckpoint(
        dirpath="/kaggle/working",
        filename="best_checkpoint",
        save_top_k=2,
        verbose=True,
        monitor="val_loss",
        mode="min"
    )

    devices = [0] if torch.cuda.is_available() else ["cpu"]

    trainer = pl.Trainer(
    callbacks=[checkpoint_callback],
    max_epochs=2,
    devices=1,
    accelerator="gpu",
    precision = 16,
    accumulate_grad_batches=4,
)




    # Train the model
    trainer.fit(models, dataModule)

run()



'''
#!git clone https:'''//huggingface.co/meta-llama/Llama-2-7b
# Model from Hugging Face hub
base_model = "meta-llama/Llama-2-7b-chat-hf"


# Fine-tuned model
new_model = "llama-2-7b-chat-space"

'''

#load model
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
'''
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
# Add a padding token to the tokenizer
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Load model with torch_dtype specified
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16)

# Assume 'kepoi_name' column contains the text you want to tokenize
def tokenize_function(example):
    # Access the 'kepoi_name' column for tokenization
    # example is now a string from the "kepoi_name" column
    # Ensure all token IDs are within the valid vocabulary range
    max_length = 128  # You can adjust this value based on your data and model

    tokens = tokenizer(
        example,
        padding="max_length",
        truncation=True,
        max_length=max_length  # Add max_length here
    )
    tokens['input_ids'] = [id % tokenizer.vocab_size for id in tokens['input_ids']]  # Wrap out-of-range IDs
    return tokens

# Apply to the 'kepoi_name' column of the DataFrame
tokenized_data = df['kepoi_name'].map(tokenize_function)

# Convert the tokenized data to a DataFrame if needed
tokenized_datasets = pd.DataFrame(list(tokenized_data))
'''

